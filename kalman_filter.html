<!DOCTYPE html>
<html>
<head>
    <title>Noah Chaim Freedman</title>
    <meta charset="UTF-8">
    <meta name="description" content="Noah Chaim Freedman's website">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Noah Chaim Freedman">
    <link rel="icon" href="cube.png">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="sty.css">
    <script
        src="https://code.jquery.com/jquery-3.3.1.js"
        integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
        crossorigin="anonymous">
    </script>
    <script>
        $(function(){$("#header").load("header.html");});
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
        }
        });
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body class="w3-white w3-content" style="max-width:1600px">

<div id="header"></div>

<div class="w3-main" style="margin-left:15%;margin-right:15%;text-align:justify">
    <br><br>
    <div class="w3-container">
        <h1>Kalman Filter</h1>
    </div>

    <div class="w3-container txt3" style="margin-bottom:32px">
The Kalman filter is a model and decoding method for linear dynamical systems of latent variables coupled with a linear observation model. This discussion will pertain to systems with mean \(0\) latent variables \(z\) and observables \(x\). Specifically, let's say we have the following state evolution model<br><br>

\[
z_1\sim N(\pi, V)\\
z_t|z_{t-1}\sim N(Az_{t-1}, Q)
\]<br>

along with an observation model<br><br>

\[x_t|z_t\sim N(Cz_t, R)\]<br>

Given a set of observations \(\{x\}_1^N\), or more generally a set of a set of observations (I won't discuss this here for notational simplicity), we can estimate our model parameters using the EM algorithm. If we have access to the latent variables, we can explicitly solve for the parameters by computing derivatives of the log-likelihood.<br><br>

Given fitted parameters, latent variables may now be estimated from observations by maximizing \(P(z_t|\{x\}_1^t)\) w.r.t. \(z_t\). We may obtain this distribution via the following iterative procedure:<br><br>

\[
Given: P(z_0)\\
P(z_t|\{x\}_1^{t-1})=\int P(z_t|z_{t-1})P(z_{t-1}|\{x\}_1^{t-1})dz_{t-1}\\
P(z_t|\{x\}_1^t)=\frac{P(x_t|z_t)P(z_t|\{x\}_1^{t-1})}{P(x_t|\{x\}_1^{t-1})}
\]<br>

Since all \(z_t\) and \(x_t\) are Gaussian, their joint and conditional distributions are Gaussian. Thus, computing the mean of \(P(z_t|\{x\}_1^t)\) will yeild the best prediction of \(z_t\) at time \(t\). Our solution will be recursive and it will help to define<br><br>

\[
\mu_t^\tau=E[z_t|\{x\}_1^\tau]\\
\Sigma_t^\tau=cov[z_t|\{x\}_1^\tau]
\]<br>

We can now get expressions for \(\mu_t^{t-1}\) and \(\Sigma_t^{t-1}\), which fully specify \(P(z_t|\{x\}_1^{t-1})\) above, as follows.<br><br>

\[z_t=Az_{t-1}+v\:,\quad v\sim N(0,Q)\]
\begin{equation*}
\begin{aligned}
E[z_t]&=AE[z_{t-1}]\\
E[z_t|\{x\}_1^{t-1}]&=AE[z_{t-1}|\{x\}_1^{t-1}]\\
\mu_t^{t-1}&=A\mu_{t-1}^{t-1}\\\\
cov[z_t]&=Acov[z_{t-1}]A^T+Q\\
cov[z_t|\{x\}_1^{t-1}]&=Acov[z_{t-1}|\{x\}_1^{t-1}]A^T+Q\\
\Sigma_t^{t-1}&=A\Sigma_{t-1}^{t-1}A^T+Q
\end{aligned}
\end{equation*}<br>

Expressions for \(\mu_t^{t}\) and \(\Sigma_t^{t}\), which fully specify \(P(z_t|\{x\}_1^{t})\) above, are derived in a similar fashion. First, we derive the expectation and covariance of \(x|\{x\}_1^{t-1}\) and the cross-covariance of \(x_t\) and \(z_t\) conditioned on \(\{x\}_1^{t-1}\). This will allow us to use results from Gaussian conditioning to derive expressions later.<br><br>

\[x_t=Cz_{t}+w\:,\quad w\sim N(0,R)\]
\begin{equation*}
\begin{aligned}
E[x_t]&=CE[z_t]\\
E[x_t|\{x\}_1^{t-1}]&=CE[z_t|\{x\}_1^{t-1}]\\
&=C\mu_t^{t-1}\\\\
cov[x_t]&=Ccov[z_t]C^T+R\\
cov[x_t|\{x\}_1^{t-1}]&=Ccov[z_t|\{x\}_1^{t-1}]C^T+R\\
&=C\Sigma_t^{t-1}A^T+R\\\\
\end{aligned}
\end{equation*}
\begin{equation*}
E[x_tz_t^T|\{x\}_t^{t-1}]-E[x_t|\{x\}_1^{t-1}]E[z_t|\{x\}_1^{t-1}]^T\\=E[Cz_tz_t^T+w_tz_t^T|\{x\}_1^{t-1}]-C\mu_t^{t-1}(\mu_t^{t-1})^T\\=C\Sigma_t^{t-1}+C\mu_t^{t-1}(\mu_t^{t-1})^T-C\mu_t^{t-1}(\mu_t^{t-1})\\=C\Sigma_t^{t-1}
\end{equation*}<br>

Now we have the full expectation and variance for the joint \(x_t\) and \(z_t\) distribution and can obtain the following using results from Gaussian conditioning (see 2006 Bishop 2.3.1)<br><br>

\begin{equation*}
\begin{aligned}
\mu_t^t&=E[z_t|x_t,\{x\}_1^{t-1}\\
&=\mu_t^{t-1}+\Sigma_t^{t-1}C^T(C\Sigma_t^{t-1}C^T+R)^{-1}(x_t-C\mu_t^{t-1})\\
&=\mu_t^{t-1}+K_t(x_t-C\mu_t^{t-1})\\\\
\Sigma_t^t&=cov[z_t|x_t,\{x\}_1^{t-1}]\\
&=\Sigma_t^{t-1}-\Sigma_t^{t-1}C^T(C\Sigma_t^{t-1}C^T+R)^{-1}C\Sigma_t^{t-1}\\
&=\Sigma_t^{t-1}-K_tC\Sigma_t^{t-1}
\end{aligned}
\end{equation*}<br>

Now, we may proceed with estimation via the following iterative scheme:<br><br>
\begin{equation*}
\begin{aligned}
initialize:\mu_1^0&=\pi,\;\Sigma_1^0=V\\
\mu_t^{t-1}&=A\mu_{t-1}^{t-1}\\
\Sigma_t^{t-1}&=A\Sigma_{t-1}^{t-1}A^T+Q\\
\mu_t^t&=\mu_t^{t-1}+K_t(x_t-C\mu_t^{t-1})\\
\Sigma_t^t&=\Sigma_t^{t-1}-K_tC\Sigma_t^{t-1}\\
\end{aligned}
\end{equation*}
(<a href="https://course.ece.cmu.edu/~ece698/">source</a>)
    </div>
</div>
</body>
</html>
