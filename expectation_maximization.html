<!DOCTYPE html>
<html>
<head>
    <title>Noah Chaim Freedman</title>
    <meta charset="UTF-8">
    <meta name="description" content="Noah Chaim Freedman's website">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Noah Chaim Freedman">
    <link rel="icon" href="cube.png">
    <link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Raleway">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="sty.css">
    <script
        src="https://code.jquery.com/jquery-3.3.1.js"
        integrity="sha256-2Kok7MbOyxpgUVvAk/HJ2jigOSYS2auK4Pfzbm7uH60="
        crossorigin="anonymous">
    </script>
    <script>
        $(function(){$("#header").load("header.html");});
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
        }
        });
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>

<body class="w3-white w3-content" style="max-width:1600px">

<div class="me_main w3-main" style="text-align:justify">
    <div class="w3-container">
        <h1>Expectation Maximization</h1>
    </div>
    <div class="w3-container txt3" style="margin-bottom:32px">
EM algorithms are extremely useful schemes in unsupervised learning. Let's say we have a set of observations \(\{x_i\}_{i=1}^N\) and we want to fit a model with latent variables \(\textbf{z}\) and parameters \(\theta\). Our log-likelihood function takes the form<br><br>

\[
log(p(\textbf x|\theta))=\sum_{i=1}^{N}log(p(x_i|\theta))=\sum_{i=1}^{N}log\Big(\sum_{\textbf{z}}p(x_i, \textbf{z}|\theta)\Big)
\]<br>

Now, we can introduce an arbitrary set of distributions \(\{Q_i(\textbf{z})\}_{i=1}^{N}\) and use Jensen's inequality to obtain a loose lower bound on the log-likelihood as follows<br><br>

\[
\sum_{i=1}^{N}log\Big(\sum_{\textbf{z}}Q_i(\textbf{z})\frac{p(x_i, \textbf{z}|\theta)}{Q_i(\textbf{z})}\Big)\geq\sum_{i=1}^{N}\sum_{\textbf{z}}Q_i(\textbf{z})log\Big(\frac{p(x_i, \textbf{z}|\theta)}{Q_i(\textbf{z})}\Big)
\]<br>

For a fixed \(\theta\) we can make this bound tight by setting \(Q_i(\textbf{z})\propto p(x_i,\textbf{z}|\theta)\) as<br><br>

\[
Q_i(\textbf{z})=\frac{p(x_i,\textbf{z}|\theta)}{\sum_{\widetilde{\textbf{z}}}p(x_i,\widetilde{\textbf{z}}|\theta)}=\frac{p(x_i,\textbf{z}|\theta)}{p(x_i|\theta)}=p(\textbf{z}|x_i,\theta)
\]<br>

We can now obtain new estimates of our parameters by fixing \(\textbf{Q}\) and optimizing this lower bound w.r.t. \(\theta\). Given our updated \(\theta\), we proceed by constructing a new lower bound with which we can once again update our \(\theta\) in the same manner. Note that since the \(Q\) are indepent of \(\theta\) during the optimization step, we can recast optimizing the lower bound as<br><br>

\[
\arg\max_{\theta}\sum_{i=1}^{N}\sum_{\textbf{z}}Q_i(\textbf{z})log\Big(\frac{p(x_i, \textbf{z}|\theta)}{Q_i(\textbf{z})}\Big)=\arg\max_{\theta}\sum_{i=1}^{N}\sum_{\textbf{z}}Q_i(\textbf{z})log(p(x_i, \textbf{z}|\theta))
\]<br>

This procedure is iterated as such until we obtain convergence in \(\theta\) or in the log-likelihood. This algorithm monotonically increases in the log-likelihood but will only guarantee a local maximum.<br><br>

<h2>Alternative Lower Bound Derivation</h2><br>

We define a function \(\mathcal{L}(Q,\theta)\) and show how it is related to \(log(p(x,\theta))\).<br><br>

\begin{equation*}
\begin{aligned}
\mathcal{L}(\textbf{Q},\theta){} &=\sum_{i=1}^{N}\sum_{\textbf{z}}Q_i(\textbf{z})log\Big(\frac{p(\textbf{z}|x_i,\theta)p(x_i|\theta)}{Q_i(\textbf{z})}\Big)\\
&=\sum_{i=1}^{N}\sum_{\textbf{z}}Q_i(\textbf{z})log\Big(\frac{p(\textbf{z}|x_i,\theta)}{Q_i(\textbf{z})}\Big)+\sum_{i=1}^{N}\sum_{\textbf{z}}Q_i(\textbf{z})log(p(x_i|\theta))\\
&=\sum_{i=1}^{N}\sum_{\textbf{z}}Q_i(\textbf{z})log\Big(\frac{p(\textbf{z}|x_i,\theta)}{Q_i(\textbf{z})}\Big)+\sum_{i=1}^{N}log(p(x_i|\theta))\\
&=-\sum_{i=1}^{N}KL(Q_i(\textbf{z})||p(\textbf{z}|x_i,\theta)) + log(p(\textbf{x}|\theta))
\end{aligned}
\end{equation*}<br>

Now we can rearrange to get another angle on the lower bound we used above.<br><br>

\begin{equation*}
\begin{aligned}
log(p(\textbf{x}|\theta))=\mathcal{L}(Q(\textbf{z}),\theta)+\sum_{i=1}^{N}KL(Q_i(\textbf{z})||p(\textbf{z}|x_i,\theta))
\end{aligned}
\end{equation*}<br>

This form also has a nice interpretation for the starting point of Jensen's inequality, where the inequality sign is obtained by removing the \(KL\) term which is always \(\geq0\). This also further explains our choice of \(\textbf{Q}\) at each iteration to make tight the lower bound, as that choice minimizes the \(KL\) terms.
    </div>    
</div>
</body>
</html>